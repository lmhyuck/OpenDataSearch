import os
import json
import logging
from typing import Dict, Any, Optional
from dotenv import load_dotenv
from kafka import KafkaConsumer
from elasticsearch import Elasticsearch, helpers
from pydantic import BaseModel, Field, ValidationError, ConfigDict

# 1. ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)
logger = logging.getLogger("MarketStreamProcessor")

load_dotenv()

# 2. ì „ì—­ ìŠ¤í‚¤ë§ˆ ì •ì˜ (Collectorì˜ ì›ë³¸ í•„ë“œë¥¼ í‘œì¤€ í•„ë“œëª…ìœ¼ë¡œ ë§¤í•‘)
class MarketData(BaseModel):
    model_config = ConfigDict(populate_by_name=True)

    # [ë§¤í•‘] ì›ë³¸(alias) -> ì‹œìŠ¤í…œ í‘œì¤€ í•„ë“œëª…
    market_id: str = Field(..., alias="bizesId")
    market_name: str = Field(..., alias="bizesNm")
    address_road: str = Field(..., alias="rdnmAdr")
    address_jibun: str = Field(..., alias="lnoAdr")
    latitude: str = Field(..., alias="lat")
    longitude: str = Field(..., alias="lon")
    items_sold: str = Field("ì •ë³´ì—†ìŒ", alias="indsLclsNm")

    # Collectorì—ì„œ ì¶”ê°€í•œ ë©”íƒ€ë°ì´í„°
    region_code: Optional[str] = None
    collected_at: Optional[str] = None
    source: Optional[str] = "SBiz_API"

# 3. í™˜ê²½ ë³€ìˆ˜ ë° ì¸í”„ë¼ ì„¤ì •
KAFKA_SERVER = os.getenv('KAFKA_SERVER', '127.0.0.1:9092')
KAFKA_TOPIC = os.getenv('KAFKA_TOPIC', 'market-data')
ES_SERVER = os.getenv('ES_SERVER', 'http://127.0.0.1:9200')
ES_INDEX = "market-info"

try:
    consumer = KafkaConsumer(
        KAFKA_TOPIC,
        bootstrap_servers=[KAFKA_SERVER],
        group_id=os.getenv('KAFKA_GROUP_ID', 'market-processor-group-vfinal'),
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        value_deserializer=lambda x: json.loads(x.decode('utf-8'))
    )

    es = Elasticsearch([ES_SERVER], request_timeout=30)
    if not es.ping():
        raise ConnectionError("Elasticsearch ì—°ê²° ì‹¤íŒ¨")
    
    logger.info("âœ… Processor ì¸í”„ë¼ ì—°ê²° ì™„ë£Œ (Kafka & ES)")
except Exception as e:
    logger.critical(f"âŒ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    exit(1)

def transform_and_index(messages):
    """ë©”ì‹œì§€ ë¬¶ìŒì„ ë³€í™˜í•˜ì—¬ ESì— ë²Œí¬ ì¸ë±ì‹± (ì¤‘ë³µ ë°©ì§€ í¬í•¨)"""
    actions = []
    for msg_value in messages:
        try:
            # Pydanticì„ í†µí•œ ë°ì´í„° ê²€ì¦ ë° í•„ë“œëª… ì •ê·œí™”
            validated = MarketData(**msg_value)
            doc = validated.model_dump()
            
            # ES Bulk í¬ë§· ìƒì„± (idë¥¼ market_idë¡œ ì§€ì •í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)
            actions.append({
                "_op_type": "index",
                "_index": ES_INDEX,
                "_id": validated.market_id,
                "_source": doc
            })
        except ValidationError as ve:
            logger.warning(f"âš ï¸ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ìŠ¤í‚µ")
        except Exception as e:
            logger.error(f"âŒ ê°œë³„ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    if actions:
        helpers.bulk(es, actions)
        logger.info(f"ğŸš€ {len(actions)}ê±´ì˜ ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ (Upsert)")

def main():
    logger.info("ğŸ“¡ ìŠ¤íŠ¸ë¦¼ í”„ë¡œì„¸ì‹± ê°€ë™ ì¤‘...")
    batch = []
    try:
        for message in consumer:
            batch.append(message.value)
            
            # 100ê±´ì”© ëª¨ì•„ì„œ ì²˜ë¦¬í•˜ê±°ë‚˜ 5ì´ˆë§ˆë‹¤ ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™”)
            if len(batch) >= 100:
                transform_and_index(batch)
                batch = []
                
    except KeyboardInterrupt:
        logger.info("ì •ì§€ ëª…ë ¹ ìˆ˜ì‹ ")
    finally:
        if batch:
            transform_and_index(batch)
        consumer.close()
        logger.info("ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()